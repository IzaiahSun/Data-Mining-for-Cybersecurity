{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dga_file=\"/Users/leeyn/desktop/work/prac/dga.txt\"\n",
    "alexa_file=\"/Users/leeyn/desktop/work/prac/top-1m.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alexa():\n",
    "    x=[]\n",
    "    data = pd.read_csv(alexa_file, sep=\",\",header=None)\n",
    "    x=[i[1] for i in data.values]\n",
    "    return x\n",
    "\n",
    "def load_dga():\n",
    "    x=[]\n",
    "    data = pd.read_csv(dga_file, sep=\"\\t\", header=None,\n",
    "                      skiprows=18)\n",
    "    x=[i[1] for i in data.values]\n",
    "    return x\n",
    "\n",
    "def get_feature_charseq():\n",
    "    alexa=load_alexa()\n",
    "    dga=load_dga()\n",
    "    x=alexa+dga\n",
    "    max_features=10000\n",
    "    y=[0]*len(alexa)+[1]*len(dga)\n",
    "\n",
    "    t=[]\n",
    "    for i in x:\n",
    "        v=[]\n",
    "        for j in range(0,len(i)):\n",
    "            v.append(ord(i[j]))\n",
    "        t.append(v)\n",
    "\n",
    "    x=t\n",
    "    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.4)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def get_aeiou(domain):\n",
    "    count = len(re.findall(r'[aeiou]', domain.lower()))\n",
    "    count = (0.0 + count) / len(domain)\n",
    "    return count\n",
    "\n",
    "def get_uniq_char_num(domain):\n",
    "    count=len(set(domain))\n",
    "    count=(0.0+count)/len(domain)\n",
    "    return count\n",
    "\n",
    "def get_uniq_num_num(domain):\n",
    "    count = len(re.findall(r'[1234567890]', domain.lower()))\n",
    "    count = (0.0 + count) / len(domain)\n",
    "    return count\n",
    "\n",
    "#单纯的文本特征\n",
    "def get_feature():\n",
    "    from sklearn import preprocessing\n",
    "    alexa=load_alexa()\n",
    "    dga=load_dga()\n",
    "    v=alexa+dga\n",
    "    y=[0]*len(alexa)+[1]*len(dga)\n",
    "    x=[]\n",
    "\n",
    "    for vv in v:\n",
    "        vvv=[get_aeiou(vv),get_uniq_char_num(vv),get_uniq_num_num(vv),len(vv)]\n",
    "        x.append(vvv)\n",
    "\n",
    "    x=preprocessing.scale(x)\n",
    "    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.4)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# 2-gram特征提取\n",
    "def get_feature_2gram():\n",
    "    alexa=load_alexa()\n",
    "    dga=load_dga()\n",
    "    x=alexa+dga\n",
    "    max_features=10000\n",
    "    y=[0]*len(alexa)+[1]*len(dga)\n",
    "\n",
    "    CV = CountVectorizer(\n",
    "                                    ngram_range=(2, 2),\n",
    "                                    token_pattern=r'\\w',\n",
    "                                    decode_error='ignore',\n",
    "                                    strip_accents='ascii',\n",
    "                                    max_features=max_features,\n",
    "                                    stop_words='english',\n",
    "                                    max_df=1.0,\n",
    "                                    min_df=1)\n",
    "    x = CV.fit_transform(x)\n",
    "    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.4)\n",
    "\n",
    "    return x_train.toarray(), x_test.toarray(), y_train, y_test\n",
    "\n",
    "#2，3，4进行分\n",
    "def get_feature_234gram():\n",
    "    alexa=load_alexa()\n",
    "    dga=load_dga()\n",
    "    x=alexa+dga\n",
    "    max_features=10000\n",
    "    y=[0]*len(alexa)+[1]*len(dga)\n",
    "\n",
    "    CV = CountVectorizer(\n",
    "                                    ngram_range=(2, 4),\n",
    "                                    token_pattern=r'\\w',\n",
    "                                    decode_error='ignore',\n",
    "                                    strip_accents='ascii',\n",
    "                                    max_features=max_features,\n",
    "                                    stop_words='english',\n",
    "                                    max_df=1.0,\n",
    "                                    min_df=1)\n",
    "    x = CV.fit_transform(x)\n",
    "    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.4)\n",
    "\n",
    "    return x_train.toarray(), x_test.toarray(), y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyn/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "def do_mlp(x_train, x_test, y_train, y_test):\n",
    "    global max_features\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    joblib.dump(clf,\"mlp.m\")\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "def do_xgboost(x_train, x_test, y_train, y_test):\n",
    "    xgb_model = xgb.XGBClassifier().fit(x_train, y_train)\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text feature & xgboost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87      3976\n",
      "           1       0.90      0.82      0.86      3970\n",
      "\n",
      "    accuracy                           0.87      7946\n",
      "   macro avg       0.87      0.87      0.87      7946\n",
      "weighted avg       0.87      0.87      0.87      7946\n",
      "\n",
      "[[3628  348]\n",
      " [ 713 3257]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyn/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81      3952\n",
      "           1       0.78      0.89      0.84      3994\n",
      "\n",
      "    accuracy                           0.82      7946\n",
      "   macro avg       0.83      0.82      0.82      7946\n",
      "weighted avg       0.83      0.82      0.82      7946\n",
      "\n",
      "[[2973  979]\n",
      " [ 422 3572]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"text feature & xgboost\")\n",
    "    x_train, x_test, y_train, y_test = get_feature()\n",
    "    do_xgboost(x_train, x_test, y_train, y_test)\n",
    "    print(\"2gram & xgboost\")\n",
    "    x_train, x_test, y_train, y_test = get_feature_2gram()\n",
    "    do_xgboost(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    #do_nb(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyn/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.82      3852\n",
      "           1       0.80      0.90      0.85      4094\n",
      "\n",
      "    accuracy                           0.83      7946\n",
      "   macro avg       0.84      0.83      0.83      7946\n",
      "weighted avg       0.84      0.83      0.83      7946\n",
      "\n",
      "[[2938  914]\n",
      " [ 412 3682]]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = get_feature_234gram()\n",
    "do_xgboost(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2gram & MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyn/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      3959\n",
      "           1       0.96      0.94      0.95      3987\n",
      "\n",
      "    accuracy                           0.95      7946\n",
      "   macro avg       0.95      0.95      0.95      7946\n",
      "weighted avg       0.95      0.95      0.95      7946\n",
      "\n",
      "[[3792  167]\n",
      " [ 226 3761]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "print(\"2gram & MLP\")\n",
    "x_train, x_test, y_train, y_test = get_feature_2gram()\n",
    "do_mlp(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
