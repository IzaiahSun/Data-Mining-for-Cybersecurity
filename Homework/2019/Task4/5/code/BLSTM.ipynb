{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "dga_file=\"/Users/leeyn/desktop/work/prac/dga.txt\"\n",
    "alexa_file=\"/Users/leeyn/desktop/work/prac/top-1m.csv\"\n",
    "\n",
    "def load_alexa():\n",
    "    x=[]\n",
    "    data = pd.read_csv(alexa_file, sep=\",\",header=None)\n",
    "    x=[i[1] for i in data.values]\n",
    "    return x\n",
    "\n",
    "def load_dga():\n",
    "    x=[]\n",
    "    data = pd.read_csv(dga_file, sep=\"\\t\", header=None)\n",
    "    x=[i[1] for i in data.values]\n",
    "    return x\n",
    "\n",
    "good_data = load_alexa()\n",
    "bad_data= load_dga()\n",
    "\n",
    "\n",
    "data = []\n",
    "data.extend(good_data)\n",
    "data.extend(bad_data)\n",
    "\n",
    "labels = []\n",
    "labels.extend([0] * len(good_data))\n",
    "labels.extend([1] * len(bad_data))\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "token_path = 'tokenizer.pkl'\n",
    "pickle.dump(tokenizer, open(token_path, 'wb'))\n",
    "\n",
    "index = [i for i in range(len(data))]\n",
    "random.shuffle(index)\n",
    "data = np.array(data)[index]\n",
    "labels = np.array(labels)[index]\n",
    "\n",
    "TRAIN_SIZE = int(0.8 * len(data))\n",
    "\n",
    "X_train, X_test = data[0:TRAIN_SIZE], data[TRAIN_SIZE:]\n",
    "Y_train, Y_test = labels[0:TRAIN_SIZE], labels[TRAIN_SIZE:]\n",
    "\n",
    "session = tf.Session()\n",
    "K.set_session(session)\n",
    "\n",
    "\n",
    "QA_EMBED_SIZE = 64\n",
    "DROPOUT_RATE = 0.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 100)          3900      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,961\n",
      "Trainable params: 96,831\n",
      "Non-trainable params: 130\n",
      "_________________________________________________________________\n",
      "Train on 11134 samples, validate on 4772 samples\n",
      "Epoch 1/10\n",
      "11134/11134 [==============================] - 95s 9ms/step - loss: 0.4523 - acc: 0.8563 - val_loss: 0.3063 - val_acc: 0.9396\n",
      "Epoch 2/10\n",
      "11134/11134 [==============================] - 87s 8ms/step - loss: 0.3785 - acc: 0.9340 - val_loss: 0.3305 - val_acc: 0.9394\n",
      "Epoch 3/10\n",
      "11134/11134 [==============================] - 89s 8ms/step - loss: 0.3486 - acc: 0.9454 - val_loss: 0.3492 - val_acc: 0.9472\n",
      "Epoch 4/10\n",
      "11134/11134 [==============================] - 92s 8ms/step - loss: 0.3304 - acc: 0.9479 - val_loss: 0.3176 - val_acc: 0.9554\n",
      "Epoch 5/10\n",
      "11134/11134 [==============================] - 92s 8ms/step - loss: 0.3169 - acc: 0.9502 - val_loss: 0.3394 - val_acc: 0.9520\n",
      "Epoch 6/10\n",
      "11134/11134 [==============================] - 92s 8ms/step - loss: 0.3061 - acc: 0.9499 - val_loss: 0.3197 - val_acc: 0.9547\n",
      "Epoch 7/10\n",
      "11134/11134 [==============================] - 91s 8ms/step - loss: 0.2970 - acc: 0.9504 - val_loss: 0.3009 - val_acc: 0.9516\n",
      "Epoch 8/10\n",
      "11134/11134 [==============================] - 92s 8ms/step - loss: 0.2885 - acc: 0.9526 - val_loss: 0.3026 - val_acc: 0.9510\n",
      "Epoch 9/10\n",
      "11134/11134 [==============================] - 92s 8ms/step - loss: 0.2790 - acc: 0.9551 - val_loss: 0.2748 - val_acc: 0.9545\n",
      "Epoch 10/10\n",
      "11134/11134 [==============================] - 92s 8ms/step - loss: 0.2709 - acc: 0.9537 - val_loss: 0.2786 - val_acc: 0.9562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x64188eda0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM,\n",
    "                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(\n",
    "    Bidirectional(LSTM(QA_EMBED_SIZE, return_sequences=False, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)))\n",
    "model.add(Dense(QA_EMBED_SIZE))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64 * 4\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'model-blstm.h5', save_best_only=True, save_weights_only=False)\n",
    "tensor_board = TensorBoard(\n",
    "    'log/tflog-blstm', write_graph=True, write_images=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "          validation_split=VALIDATION_SPLIT, shuffle=True,\n",
    "          callbacks=[early_stopping, model_checkpoint, tensor_board])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      2018\n",
      "           1       0.95      0.96      0.95      1959\n",
      "\n",
      "    accuracy                           0.95      3977\n",
      "   macro avg       0.95      0.95      0.95      3977\n",
      "weighted avg       0.95      0.95      0.95      3977\n",
      "\n",
      "[[1923   95]\n",
      " [  85 1874]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "y_predict_list = model.predict(X_test)\n",
    "y_predict = []\n",
    "for i in y_predict_list:\n",
    "  \n",
    "    if i[0] > 0.5:\n",
    "        y_predict.append(1)\n",
    "    else:\n",
    "        y_predict.append(0)\n",
    "\n",
    "print(classification_report(Y_test, y_predict))\n",
    "print(metrics.confusion_matrix(Y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3977/3977 [==============================] - 9s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2809591689450315, 0.9547397544973305]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, verbose=1, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
