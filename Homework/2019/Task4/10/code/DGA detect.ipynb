{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split#分割数据集\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score   #准确率accuracy\n",
    "from sklearn.metrics import precision_score  #精确率precision\n",
    "from sklearn.metrics import recall_score     #召回率recall\n",
    "from collections import Counter              #统计list中各个元素出现的次数\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = 'results.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_features, maxlen):\n",
    "    \"\"\"Build LSTM model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(max_epoch=5, nfolds=10, batch_size=128):\n",
    "    \"\"\"Run train/test on logistic regression model\"\"\"\n",
    "    dataSet = pd.read_csv(\"data.csv\",header=None)\n",
    "\n",
    "\t#提取数据和标签\n",
    "    X = dataSet[0].values\n",
    "    y = dataSet[1].values\n",
    "\n",
    "    # Generate a dictionary of valid characters\n",
    "\t#生成一个有效字符的字典\n",
    "\t#将每个字符串转换为表示每个可能字符的int数组。这种编码是任意的，\n",
    "\t#但是应该从1开始（我们为结束序列token保留0）并且是连续的。\n",
    "    valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}\n",
    "\n",
    "    max_features = len(valid_chars) + 1\n",
    "    maxlen = np.max([len(x) for x in X])\n",
    "\n",
    "    # Convert characters to int and pad\n",
    "    X = [[valid_chars[y] for y in x] for x in X]\n",
    "    X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "\n",
    "    lstm_data = []\n",
    "\n",
    "    #每循环一次就生成一个新的模型\n",
    "    for fold in range(nfolds):\n",
    "        print(\"fold %u/%u\" % (fold+1, nfolds))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train_number = Counter(y_train)  #训练集里DGA域名和普通域名的数量\n",
    "        test_number = Counter(y_test)   #测试集里DGA域名和普通域名的数量,1代表DGA域名，0代表普通域名\n",
    "\n",
    "\n",
    "        print('Build model...')\n",
    "        model = build_model(max_features, maxlen)\n",
    "\n",
    "        print(\"Train...\")\n",
    "        X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)\n",
    "        \n",
    "        best_iter = -1\n",
    "        best_auc = 0.0\n",
    "        out_data = {}\n",
    "\n",
    "        #同一个模型训练多次，用来提高精确率的\n",
    "        for ep in range(max_epoch):\n",
    "            model.fit(X_train, y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "            t_probs = model.predict_proba(X_holdout)\n",
    "            t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)\n",
    "\n",
    "            print('Epoch %d: auc = %f (best=%f)' % (ep, t_auc, best_auc))\n",
    "\n",
    "            if t_auc > best_auc:\n",
    "                best_auc = t_auc\n",
    "                best_iter = ep\n",
    "\n",
    "                probs = model.predict_proba(X_test)\n",
    "                for i in range(len(probs)):\n",
    "                    probs[i]=probs[i][0]\n",
    "                    if probs[i]>0.5:\n",
    "                        probs[i]=1\n",
    "                    else:\n",
    "                        probs[i]=0\n",
    "                #print(probs)\n",
    "                accuracy = accuracy_score(y_test,probs)\n",
    "                precision = precision_score(y_test,probs)\n",
    "                recall = recall_score(y_test,probs)\n",
    "                out_data = {'train_number':train_number, 'test_number':test_number, 'accuracy': accuracy,\n",
    "                            'precision': precision,'recall':recall}\n",
    "                print(out_data)\n",
    "            else:\n",
    "                # No longer improving...break and calc statistics\n",
    "                if (ep-best_iter) > 1:\n",
    "                    break\n",
    "\n",
    "        lstm_data.append(out_data)\n",
    "    results = {'valid_chars':valid_chars,'maxlen':maxlen,'final_data':lstm_data}\n",
    "    pickle.dump(results, open(RESULT_FILE, 'wb'))\n",
    "    model.save(\"model.h5\")\n",
    "    return lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1/1\n",
      "Build model...\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train...\n",
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "13300/13300 [==============================] - 19s 1ms/step - loss: 0.2811\n",
      "Epoch 0: auc = 0.990704 (best=0.000000)\n",
      "{'precision': 0.9724381625441696, 'test_number': Counter({1: 3001, 0: 2999}), 'accuracy': 0.9455, 'train_number': Counter({0: 7001, 1: 6999}), 'recall': 0.9170276574475175}\n",
      "Epoch 1/1\n",
      "13300/13300 [==============================] - 14s 1ms/step - loss: 0.1550\n",
      "Epoch 1: auc = 0.995050 (best=0.990704)\n",
      "{'precision': 0.9514950166112957, 'test_number': Counter({1: 3001, 0: 2999}), 'accuracy': 0.9528333333333333, 'train_number': Counter({0: 7001, 1: 6999}), 'recall': 0.9543485504831722}\n",
      "Epoch 1/1\n",
      "13300/13300 [==============================] - 15s 1ms/step - loss: 0.1398\n",
      "Epoch 2: auc = 0.994576 (best=0.995050)\n"
     ]
    }
   ],
   "source": [
    "# final_data = run(max_epoch=3,nfolds=1)\n",
    "lstm_data = run(max_epoch=3,nfolds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'precision': 0.9514950166112957, 'test_number': Counter({1: 3001, 0: 2999}), 'accuracy': 0.9528333333333333, 'train_number': Counter({0: 7001, 1: 6999}), 'recall': 0.9543485504831722}]\n"
     ]
    }
   ],
   "source": [
    "print(lstm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(max_features):\n",
    "    \"\"\"Builds logistic regression model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=max_features, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2(max_epoch=50, nfolds=10, batch_size=128):\n",
    "    \"\"\"Run train/test on logistic regression model\"\"\"\n",
    "    dataSet = pd.read_csv(\"data.csv\",header=None)\n",
    "\n",
    "\t#提取数据和标签\n",
    "    X = dataSet[0].values\n",
    "    y = dataSet[1].values\n",
    "\n",
    "    # Create feature vectors\n",
    "    print (\"vectorizing data\")\n",
    "    ngram_vectorizer = feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "    count_vec = ngram_vectorizer.fit_transform(X)\n",
    "\n",
    "    max_features = count_vec.shape[1]\n",
    "\n",
    "    # Convert labels to 0-1\n",
    "    #labels = [0 if x == 'benign' else 1 for x in labels]\n",
    "\n",
    "    bigram_data = []\n",
    "\n",
    "    for fold in range(nfolds):\n",
    "        print (\"fold %u/%u\" % (fold+1, nfolds))\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(count_vec,y, test_size=0.2)\n",
    "\n",
    "        print ('Build model...')\n",
    "        model = build_model2(max_features)\n",
    "\n",
    "        print (\"Train...\")\n",
    "        X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.05)\n",
    "        best_iter = -1\n",
    "        best_auc = 0.0\n",
    "        out_data = {}\n",
    "\n",
    "        for ep in range(max_epoch):\n",
    "            model.fit(X_train.todense(), y_train, batch_size=batch_size, nb_epoch=1)\n",
    "\n",
    "            t_probs = model.predict_proba(X_holdout.todense())\n",
    "            t_auc = sklearn.metrics.roc_auc_score(y_holdout, t_probs)\n",
    "\n",
    "            print ('Epoch %d: auc = %f (best=%f)' % (ep, t_auc, best_auc))\n",
    "\n",
    "            if t_auc > best_auc:\n",
    "                best_auc = t_auc\n",
    "                best_iter = ep\n",
    "\n",
    "                probs = model.predict_proba(X_test.todense())\n",
    "\n",
    "                for i in range(len(probs)):\n",
    "                    probs[i]=probs[i][0]\n",
    "                    if probs[i]>0.5:\n",
    "                        probs[i]=1\n",
    "                    else:\n",
    "                        probs[i]=0\n",
    "                #print(probs)\n",
    "                accuracy = accuracy_score(y_test,probs)\n",
    "                precision = precision_score(y_test,probs)\n",
    "                recall = recall_score(y_test,probs)\n",
    "                out_data = { 'accuracy': accuracy,'precision': precision,'recall':recall}\n",
    "                print(out_data)\n",
    "            else:\n",
    "                # No longer improving...break and calc statistics\n",
    "                if (ep-best_iter) > 5:\n",
    "                    break\n",
    "\n",
    "        bigram_data.append(out_data)\n",
    "\n",
    "    return bigram_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing data\n",
      "fold 1/1\n",
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "15200/15200 [==============================] - 1s 58us/step - loss: 0.5789\n",
      "Epoch 0: auc = 0.986617 (best=0.000000)\n",
      "{'precision': 0.9082217973231358, 'accuracy': 0.92375, 'recall': 0.9438648782911078}\n",
      "Epoch 1/1\n",
      " 5120/15200 [=========>....................] - ETA: 0s - loss: 0.4515"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15200/15200 [==============================] - 1s 36us/step - loss: 0.4152\n",
      "Epoch 1: auc = 0.991563 (best=0.986617)\n",
      "{'precision': 0.9411182582879762, 'accuracy': 0.9425, 'recall': 0.9448584202682563}\n",
      "Epoch 1/1\n",
      " 4864/15200 [========>.....................] - ETA: 0s - loss: 0.3520"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15200/15200 [==============================] - 1s 35us/step - loss: 0.3275\n",
      "Epoch 2: auc = 0.993071 (best=0.991563)\n",
      "{'precision': 0.9518314099347717, 'accuracy': 0.947, 'recall': 0.942374565325385}\n",
      "[{'precision': 0.9518314099347717, 'accuracy': 0.947, 'recall': 0.942374565325385}]\n"
     ]
    }
   ],
   "source": [
    "bigram_data = run2(max_epoch=3,nfolds=1)\n",
    "print(bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dga_predict(domain):\n",
    "    model = load_model('model.h5')\n",
    "    results = pickle.load(open(RESULT_FILE,'rb'))\n",
    "    valid_chars = results['valid_chars']\n",
    "    maxlen = results['maxlen']\n",
    "    \n",
    "    X = [[valid_chars[y] for y in domain]]\n",
    "    X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    result = model.predict(X)\n",
    "    print(\"这个域名为恶意域名的概率为:\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个域名为恶意域名的概率为: [[0.00382655]]\n"
     ]
    }
   ],
   "source": [
    "dga_predict(\"baidu.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个域名为恶意域名的概率为: [[0.99884146]]\n"
     ]
    }
   ],
   "source": [
    "dga_predict(\"edjsdjksmxma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
