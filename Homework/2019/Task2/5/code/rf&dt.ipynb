{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfile = \"mal.csv\"\\nfile2 = \"data.csv\"\\ndef load_alexa():\\n    x=[]\\n    data = pd.read_csv(file, sep=\",\",header=None)\\n    x=[i[1] for i in data.values]\\n    for i in range(len(x) - 1, -1, -1):\\n        if x[i] == \\'-\\':\\n            x.remove(x[i])\\n    return x\\ndef load_bad(file):\\n    x=[]\\n    data = pd.read_csv(file, sep=\",\",header=None)\\n    for i in data.values:\\n        if i[1]==\\'good\\':\\n            x.append(i[0])\\n    return x\\ndef write2csv(dmlist,filename):\\n    file_write_obj = open(filename, \\'w\\')\\n    for i in dmlist:\\n        file_write_obj.writelines(i)\\n        file_write_obj.write(\\'\\n\\')\\n#write2csv(load_alexa())\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "file = \"mal.csv\"\n",
    "file2 = \"data.csv\"\n",
    "def load_alexa():\n",
    "    x=[]\n",
    "    data = pd.read_csv(file, sep=\",\",header=None)\n",
    "    x=[i[1] for i in data.values]\n",
    "    for i in range(len(x) - 1, -1, -1):\n",
    "        if x[i] == '-':\n",
    "            x.remove(x[i])\n",
    "    return x\n",
    "def load_bad(file):\n",
    "    x=[]\n",
    "    data = pd.read_csv(file, sep=\",\",header=None)\n",
    "    for i in data.values:\n",
    "        if i[1]=='good':\n",
    "            x.append(i[0])\n",
    "    return x\n",
    "def write2csv(dmlist,filename):\n",
    "    file_write_obj = open(filename, 'w')\n",
    "    for i in dmlist:\n",
    "        file_write_obj.writelines(i)\n",
    "        file_write_obj.write('\\n')\n",
    "#write2csv(load_alexa())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the good csv and the bad csv\n",
    "def load_data(file):\n",
    "    x=[]\n",
    "    data = pd.read_csv(file, sep=\"\\n\",header=None)\n",
    "    x=[i[0] for i in data.values]\n",
    "    return x\n",
    "good = load_data(\"good.csv\")\n",
    "bad = load_data(\"bad.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#计算每一个good/bad列表中的长度\n",
    "def get_len(wlist):\n",
    "    x = []\n",
    "    for i in wlist:\n",
    "        x.append(len(i))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.00109622093782\n",
      "1025\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(get_len(good)))\n",
    "print(np.max(get_len(good)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.23977454883914\n",
      "2307\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(get_len(bad)))\n",
    "print(np.max(get_len(bad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算url里面的数字个数\n",
    "def get_num(strings):\n",
    "    count = 0\n",
    "    numregx = re.compile(r'[0-9]')\n",
    "    numlist = numregx.findall(strings)\n",
    "    count = len(numlist)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算非字母数字个数\n",
    "def getNoLetter(strings):\n",
    "    count=0\n",
    "    num_regex = re.compile(r'[0-9]')\n",
    "    zimu_regex = re.compile(r'[a-zA-z]')\n",
    "    numlist = num_regex.findall(strings)\n",
    "    countNum = len(numlist)\n",
    "    zimulist = zimu_regex.findall(strings)\n",
    "    countZimu = len(zimulist)\n",
    "    count = len(strings)-countZimu-countNum\n",
    "    return count\n",
    "\n",
    "#print(good[1])\n",
    "#count3 = getNoLetter(good[1])\n",
    "#print(count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123movies.info/movie/The_Cavemans_Valentine_(2001)_1457621\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#计算目录层数\n",
    "def directoryLayer(strings):\n",
    "    count=0\n",
    "    for i in strings:\n",
    "        if i=='/':\n",
    "            count+=1\n",
    "    return count\n",
    "print(good[100])\n",
    "print(directoryLayer(good[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取恶意文本短语个数\n",
    "def get_csen(url):\n",
    "    bag=['jordan','phishing','crack','ebayisapi','banking','account']\n",
    "    cnt = 0\n",
    "    for i in bag:\n",
    "        if i in url.lower():\n",
    "            cnt = cnt+1\n",
    "    return cnt\n",
    "#获取后缀\n",
    "def get_postfix(url):\n",
    "    bag=['exe','avi','txt','dll','zip','vbs','gif']\n",
    "    cnt = 0\n",
    "    for i in bag:\n",
    "        if i in url.lower():\n",
    "            cnt+=1\n",
    "    return cnt\n",
    "\n",
    "#获取@符号\n",
    "def get_at(url):\n",
    "    cnt=0\n",
    "    if '@' in url.lower():\n",
    "        cnt+=1\n",
    "    return cnt\n",
    "#获取连接符\n",
    "def get_catch(url):\n",
    "    cnt=0\n",
    "    if '-' in url.lower():\n",
    "        cnt+=1\n",
    "    return cnt\n",
    "def get_catch1(url):\n",
    "    cnt=0\n",
    "    if '_' in url.lower():\n",
    "        cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def get_len(url):\n",
    "    return len(url)\n",
    "\n",
    "def get_request(url):\n",
    "    count = 0\n",
    "    for j in range(0,len(url)):\n",
    "        if url[j] == '?':\n",
    "            count=len(url)-j\n",
    "    return count\n",
    "\n",
    "def get_portion(url):\n",
    "    count=0\n",
    "    num_regex = re.compile(r'[0-9]')\n",
    "    zimu_regex = re.compile(r'[a-zA-z]')\n",
    "    numlist = num_regex.findall(url)\n",
    "    countNum = len(numlist)\n",
    "    zimulist = zimu_regex.findall(url)\n",
    "    countZimu = len(zimulist)\n",
    "    count = countZimu-(countNum+countZimu)\n",
    "    return count\n",
    "\n",
    "def get_portion1(url):\n",
    "    count=0\n",
    "    num_regex = re.compile(r'[a-z]')\n",
    "    zimu_regex = re.compile(r'[A-z]')\n",
    "    numlist = num_regex.findall(url)\n",
    "    countNum = len(numlist)\n",
    "    zimulist = zimu_regex.findall(url)\n",
    "    countZimu = len(zimulist)\n",
    "    count = countZimu-(countNum+countZimu)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 统计域名的最高一级域名是几级域名\n",
    "url =\"mobiles-free.org/Moncomptecpsess1022823111/8be18/bb508/moncompte/index.php?clientid=16013&amp;defaults=webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=utf-8\"\n",
    "def get_ji(url):\n",
    "    url = \"//\"+str(url)\n",
    "    url = urlparse(url)\n",
    "    url_list = url.netloc.split('.')\n",
    "    return len(url_list)-1\n",
    "print(get_ji(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "url = \"http://hi.com.sss/fhdsj.php?\"\n",
    "# 获取请求参数的总的长度\n",
    "def get_query(url):\n",
    "    url = \"//\"+str(url)\n",
    "    url = urlparse(url)\n",
    "    query = url.query\n",
    "    return len(query)\n",
    "\n",
    "print(get_csen(\"https://dfdsf,com/acc/login./banking\"))\n",
    "print(get_query(url))\n",
    "gquery_len = []\n",
    "bquery_len = []\n",
    "for i in good:\n",
    "    gquery_len.append(get_query(i))\n",
    "for i in bad:\n",
    "    bquery_len.append(get_query(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_feature():\n",
    "    from sklearn import preprocessing\n",
    "    good = load_data(\"good.csv\")\n",
    "    bad = load_data(\"bad.csv\")\n",
    "    v=good+bad\n",
    "    y=[0]*len(good)+[1]*len(bad)\n",
    "    x=[]\n",
    "\n",
    "#获取特征函数\n",
    "\n",
    "    for vv in v:\n",
    "        vvv=[get_request(vv),get_num(vv),getNoLetter(vv),directoryLayer(vv),get_csen(vv),get_ji(vv),get_query(vv),get_at(vv),get_catch(vv),get_catch1(vv),get_postfix(vv),get_len(vv),get_portion(vv),get_portion1(vv)]\n",
    "        x.append(vvv)\n",
    "\n",
    "    x=preprocessing.scale(x)\n",
    "    x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyn/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     68885\n",
      "           1       0.76      0.63      0.69     15551\n",
      "\n",
      "    accuracy                           0.89     84436\n",
      "   macro avg       0.84      0.79      0.81     84436\n",
      "weighted avg       0.89      0.89      0.89     84436\n",
      "\n",
      "[[65807  3078]\n",
      " [ 5822  9729]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyn/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     68885\n",
      "           1       0.79      0.63      0.70     15551\n",
      "\n",
      "    accuracy                           0.90     84436\n",
      "   macro avg       0.86      0.80      0.82     84436\n",
      "weighted avg       0.90      0.90      0.90     84436\n",
      "\n",
      "[[66353  2532]\n",
      " [ 5790  9761]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def do_decisionTree(x_train,x_test,y_train,y_test):\n",
    "    dt_model=DecisionTreeClassifier().fit(x_train,y_train)\n",
    "    y_pred=dt_model.predict(x_test)\n",
    "    #joblib.dump(dt_model,'dt.m')\n",
    "    print(\"Decision Tree\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def do_RF(x_train,x_test,y_train,y_test):\n",
    "    rf_model=RandomForestClassifier().fit(x_train,y_train)\n",
    "    y_pred=rf_model.predict(x_test)\n",
    "   # joblib.dump(rf_model,'rf.m')\n",
    "    print(\"RandomForestClassifier\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "           \n",
    "x_train, x_test, y_train, y_test = get_feature()\n",
    "do_decisionTree(x_train, x_test, y_train, y_test)\n",
    "do_RF(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = load_data(\"good.csv\")\n",
    "bad = load_data(\"bad.csv\")\n",
    "\n",
    "    #标记数据\n",
    "X=good+bad\n",
    "labels=[0]*len(good)+[1]*len(bad)\n",
    "  \n",
    "    # Generate a dictionary of valid characters\n",
    "valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Judge(model,url):\n",
    "    from sklearn import preprocessing\n",
    "    x =[get_request(url),get_num(url),getNoLetter(url),directoryLayer(url),get_csen(url),get_ji(url),get_query(url),get_at(url),get_catch(url),get_catch1(url),get_postfix(url),get_len(url),get_portion(url),get_portion1(url)]\n",
    "    clf = joblib.load(model)\n",
    "    ans = clf.predict(np.array(x).reshape(1,-1))\n",
    "    if ans == 1:\n",
    "        print('Bad url')\n",
    "    if ans == 0:\n",
    "        print('Good url')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good url\n",
      "Bad url\n",
      "Bad url\n",
      "Good url\n"
     ]
    }
   ],
   "source": [
    "Judge('dt.m','www.baidu.com')\n",
    "Judge('dt.m','www.crackspider.us/toolbar/install.php?pack=exe')\n",
    "Judge('rf.m','www.crackspider.us/toolbar/install.php?pack=exe')\n",
    "Judge('rf.m','www.baidu.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
