{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malicious URL detection based on lexical features\n",
    "\n",
    "    There are several types of features for a URL, BlackList, Lexical, Host-based, Content-based, etc. among which the most efficient and ecnomic way is to generate Lexical features. And thus, lexical features are what we concentrate on in this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Lexical Features\n",
    "    Traditional Lexical Features include statistical properties of the URL string, like the length of URL, length of each components of the URL (Hostname, Top Level Domain, Primary domain, etc.), the number of special characters, etc. Also, we can construct directory based on each segment delimited by a special character(e.g. \"/\", \".\", \"?\", \"=\", etc.), so-called bag-of-words model. And, in order to detect algorithmically generated malicious URLs, character level features are required, the most common way is to calculate the alpha-numeric distribution, like KL-divergence, Jaccard Coefficient, and Edit-distance using unigram and bigram distributions of characters.\n",
    "    Heuritic Features come up with the objective of being obfuscation resistant, which can be generally classified as five categories: URL-related features (keywords, length, etc.), Domain features (length of domain name, whether IP address is used as a domain name or not, etc.), Directory related features (length of directory, number of subdirectory tokens, etc.), File name features(length of filename,nubmer of delimiters, etc.), and Arguement Features (length of arguements, number of variables, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8 \"\"\"\n",
    "import urllib\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "################## 辅助URL处理 #################\n",
    "def normalize(url):\n",
    "    return '//' + url if '//' not in url else url\n",
    "\n",
    "\n",
    "#获取自//之后的内容\n",
    "def removeURLHeader(url):\n",
    "    return url.split('//', 1)[-1]\n",
    "\n",
    "################## URL统计特征 #################\n",
    "\n",
    "#获取URL长度\n",
    "def getLength(url):\n",
    "    return len(url)\n",
    "\n",
    "#是否具有@符合\n",
    "def aite(url):\n",
    "    return 1 if '@' in url else 0\n",
    "\n",
    "#含数字个数\n",
    "def getDigitsCount(url):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for i in range(len(url)):\n",
    "        if url[i] in string.digits:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "#大写字母个数\n",
    "def getCountUpcase(url):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for i in range(len(url)):\n",
    "        if url[i] in string.ascii_uppercase:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "#前缀个数\n",
    "def getPrefixCount(url):\n",
    "    prefix = ['_', '-']\n",
    "    i = 0\n",
    "    count = 0\n",
    "    for i in range(len(url)):\n",
    "        if url[i] in prefix:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "#URL中数字-字符转换频次\n",
    "def ZhuanHuanPingci(url):\n",
    "    urlwhole = removeURLHeader(url)\n",
    "    count = 0\n",
    "    length = len(urlwhole)\n",
    "    for i in range(length):\n",
    "        if urlwhole[i] in string.digits and i + 1 < length and (\n",
    "                urlwhole[i + 1] in string.ascii_lowercase or urlwhole[i + 1] in string.ascii_uppercase):\n",
    "\n",
    "            count += 1\n",
    "        else:\n",
    "            if (urlwhole[i] in string.ascii_lowercase or urlwhole[i] in string.ascii_uppercase) and i + 1 < length and urlwhole[\n",
    "                i + 1] in string.digits:\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "################## URL启发式特征 #################\n",
    "\n",
    "##### 辅助解析 #####\n",
    "#文件名\n",
    "def getFilename(url):\n",
    "    startpos = url.find('/') + 1\n",
    "    url = url[startpos:]\n",
    "\n",
    "    return url\n",
    "\n",
    "#二级域名\n",
    "def getUrlSubDomain(url):\n",
    "    host = hostname(url)\n",
    "    subDomain = '.'.join(host.split('.')[-2:])\n",
    "\n",
    "    return subDomain\n",
    "\n",
    "#提取主机名\n",
    "def hostname(url):\n",
    "    url = normalize(url)\n",
    "    parsed_result = urlparse(url)\n",
    "    a = parsed_result.netloc\n",
    "\n",
    "    return(a)\n",
    "\n",
    "\n",
    "#获取目录路径\n",
    "def getDirectory(url):\n",
    "    url = removeURLHeader(url)\n",
    "    if '/' in url:\n",
    "\n",
    "        startpos = url.index('/') + 1\n",
    "        endpos = url.rindex('/')\n",
    "        suburl = url[startpos:endpos]\n",
    "        return suburl\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "############# 启发式特征提取 #############\n",
    "\n",
    "\n",
    "### URL-related Features ###\n",
    "\n",
    "#是否含有敏感词\n",
    "def sensitiveword(url):\n",
    "    i = 0\n",
    "    flag = 0\n",
    "    sensitive = ['secure', 'account', 'webscr', 'login', 'ebayiaphi', 'signin', 'banking', 'confirm']\n",
    "    for i in range(len(sensitive)):\n",
    "        if sensitive[i] in url:\n",
    "            flag = 1\n",
    "            break\n",
    "        else:\n",
    "            flag = 0\n",
    "            continue\n",
    "    return flag\n",
    "\n",
    "#是否含有关键词\n",
    "def targetword(url):\n",
    "    url = getUrlSubDomain(url)\n",
    "    normalDomain = ['paypal.com', 'aol.com', 'qq.com', 'made-in-china.com', 'google.com', 'facebook.com', 'yahoo.com',\n",
    "                    'live.com', 'dropbox.com', 'wellsfargo.com', 'cmr.no', 'academia.edu', 'regions.com',\n",
    "                    'shrinkthislink.com', 'maximumasp.com', 'popularenlinea.com', 'readydecks.com', 'meezanbank.com',\n",
    "                    'vencorex.com', 'ketthealth.com', 'obhrmanager.com', 'bluehost.com', 'msubillings.edu',\n",
    "                    'genxgame.com', 'gripeezoffer.com', 'bek-intern.de', 'ebay.com', 'chase.com', 'revoluza.com',\n",
    "                    'dhl.com', 'flexispy.com', 'att.com', 'uwsp.edu', 'match.com', 'alnoorhospital.com', 'ourtime.com']\n",
    "    if url in normalDomain:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "### Domain-related Features ###\n",
    "\n",
    "#是否含有错误端口\n",
    "def mistakePort(url):\n",
    "    return 1 if ':' in hostname(url) else 0\n",
    "\n",
    "#主机名项数\n",
    "def termcout(url):\n",
    "    url = hostname(url)\n",
    "    url = url.split('.')\n",
    "    return len(url)\n",
    "\n",
    "\n",
    "\n",
    "### Path-related Features ###\n",
    "\n",
    "#是否路径中含有域名\n",
    "def pathhasdomin(url):\n",
    "    url = normalize(url)\n",
    "    res = urlparse(url)\n",
    "    return 1 if res.netloc in res.path else 0\n",
    "\n",
    "### Directory-related Features ###\n",
    "\n",
    "#是(1)否(0)\n",
    "def brandname(url):#商标名\n",
    "    i = 0\n",
    "    flag = 0\n",
    "    url = getDirectory(url)\n",
    "    brandnamelist = ['53.com', 'Chase', 'Microsoft', 'ANZ', 'Citibank', 'Paypal', 'AOL', 'eBay', 'USBank', 'Banamex',\n",
    "                     'E-Gold', 'Visa', 'Bankofamerica', 'Google', 'Warcraft', 'Barclays', 'HSBC', 'Westpac',\n",
    "                     'battle.net', 'LIoyds', 'Yahoo']\n",
    "    for i in range(len(brandnamelist)):\n",
    "        if brandnamelist[i] in url:\n",
    "            flag = 1\n",
    "            break\n",
    "        else:\n",
    "            flag = 0\n",
    "            continue\n",
    "    return flag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data And Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def characterize(url):\n",
    "    return [getLength(url),aite(url), pathhasdomin(url), len(hostname(url)), sensitiveword(url), mistakePort(url), brandname(url), getDigitsCount(url), getCountUpcase(url), getPrefixCount(url), termcout(url), ZhuanHuanPingci(url), targetword(url)]\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv('data.csv', header=0)  # 读取csv数据，并将第一行视为表头，返回DataFrame类型\n",
    "data = raw_data.values\n",
    "urls = list(data[::, 0])\n",
    "labels = data[::, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.where(labels == 'bad', 1, 0)\n",
    "features  = []\n",
    "for url in urls:\n",
    "    features.append(characterize(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "training cost 3.815389 seconds\n",
      "Start predicting...\n",
      "predicting cost 0.681565 seconds\n",
      "The accruacy score is 0.906518\n",
      "The recall score is 0.627397\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "# 选取33%数据作为测试集，剩余为训练集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)\n",
    "\n",
    "time_2=time.time()\n",
    "print('Start training...')\n",
    "clf = RandomForestClassifier()  # svm class   \n",
    "clf.fit(train_features, train_labels)  # training the svc model \n",
    "time_3 = time.time()\n",
    "print('training cost %f seconds' % (time_3 - time_2))\n",
    "\n",
    "print('Start predicting...')\n",
    "test_predict=clf.predict(test_features)\n",
    "time_4 = time.time()\n",
    "print('predicting cost %f seconds' % (time_4 - time_3))\n",
    "\n",
    "score = accuracy_score(test_labels, test_predict)\n",
    "score_rec = recall_score(test_labels, test_predict)\n",
    "print(\"The accruacy score is %f\" % score)\n",
    "print(\"The recall score is %f\" % score_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_up_box():\n",
    "    \"\"\"\n",
    "    使用tkinter弹出输入框输入url, 输出\n",
    "    \"\"\"\n",
    "\n",
    "    import tkinter\n",
    "\n",
    "    \n",
    "    def inputint():\n",
    "        nonlocal test_url\n",
    "        test_url = var.get().strip()\n",
    "        # print(test_url)\n",
    "        test = []\n",
    "        test.append(characterize(test_url))\n",
    "        if clf.predict(test)[0]:\n",
    "            l.config(text=\"malicious\")\n",
    "        else:\n",
    "            l.config(text=\"benign\")\n",
    "        var.set('')\n",
    "        test_url = ''\n",
    "\n",
    "    test_url = 0\n",
    "    root = tkinter.Tk(className='Here is some interpretation')  # 弹出框框名\n",
    "    root.geometry('270x60')     # 设置弹出框的大小 w x h\n",
    "\n",
    "    var = tkinter.StringVar()   # 这即是输入框中的内容\n",
    "    var.set('') # 通过var.get()/var.set() 来 获取/设置var的值\n",
    "    entry1 = tkinter.Entry(root, textvariable=var)  # 设置\"文本变量\"为var\n",
    "    entry1.pack()   # 将entry\"打上去\"\n",
    "    l =tkinter.Label(root, bg = 'yellow', width = 20, text = '')\n",
    "    l.pack()\n",
    "    btn1 = tkinter.Button(root, text='Input', command=inputint)     # 按下此按钮(Input), 触发inputint函数\n",
    "\n",
    "    # 按钮定位\n",
    "    btn1.pack(side='right')\n",
    "\n",
    "    # 上述完成之后, 开始真正弹出弹出框\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_up_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
