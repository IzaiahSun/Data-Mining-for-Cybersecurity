# Entailment with TensorFlow理解其问题及方法

## 简介

### entailment是什么？

#### 文本间的推理关系，又称为文本蕴涵关系 (Textual Entailment)，简称蕴涵(entailment)，作为一种基本的文本间语义联系，广泛存在于自然语言文本中。简单的来说文本蕴涵关系描述的是两个文本之间的推理关系，其中一个文本作为前提(premise)，另一个文本作为假设(hypothesis)，如果根据前提P能够推理得出假设H，那么就说P蕴涵H，记做P—>H。这跟一阶逻辑中的蕴涵关系是类似的。



### entailment的研究背景

#### 随着自然语言处理(Natural LanguageProcessing, NLP)领域研究的不断深入，如何让机器能够真正地理解自然语言，而不是仅仅简单地处理语句的表层信息，渐渐成为了许多学者面临的问题。实现对文本深层次理解，是自然语言处理研究最主要也是最重要的目的之一。如果将其比作是自然语言处理研究领域的一顶皇冠的话，那么基于自然语言的语义推理无疑是这顶皇冠上最璀璨的一颗明珠。因为在获取了文本的语义后，一旦获得了它们之间的推理关系，这些文本便不再互相孤立，而是彼此联系起来，构成一张语义推理网络，从而促使机器能够真正理解并应用文本的语义信息。



### entailment的知识表示与来源

#### 知识的表示形式是为了方便应用而设计的，蕴涵知识根据是否含有变量可以划分为两类：单词及短语级别的蕴涵知识(不含有变量，如“苹果 → 水果”)和模板级别的蕴涵知识(含有变量，如“X购买了Y → X拥有Y”)。事实上，蕴涵知识的应用场景往往是特定的，很少有放之四海而皆准的蕴涵知识。例如，“acquire”作为及物动词既有“购买”的意思，也有“学习”的意思，蕴涵知识“X acquire Y  → X purchase Y” 在“AT&T acquire(收购)T-Mobile → AT&T purchase T-Mobile”的上下文中成立，但在“Children acquire(习得)skills → Children purchase skills”的场景下中并不成立，因此如何对蕴涵知识的应用场景进行建模是知识表示问题中需要考虑的地方。

#### 蕴涵知识的潜在来源有很多，例如词典、百科、新闻语料、普通互联网文本等等。按照是否有专家参与构建可以把知识源分为人工构建的资源和大规模语料两类，前者小而精，后者广而粗，针对不同的知识来源需要设计不同的知识获取方法。



### entailment使用例子

![png1](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/1.png)

#### 这个例子中前提P是“A dog jumping for a Frisbee in the snow”，意思一只狗在雪地中接飞盘玩，同时下面给出了三个假设，这三个假设中第一个跟前提是蕴涵关系(entailment)，因为这句话描述的是“一个动物正在寒冷室外玩塑料玩具”，这是能够从前提中推理出来的；而第二句化描述的是“一只猫用它的前爪来洗脸和胡子”，这跟前提是冲突的(contradiction)；第三句话则描述为“一只宠物正在和它的主人玩捡东西的游戏”，其与前提既不是蕴涵关系也没有冲突，我们把它定义成中立的(neutral)。

#### 文本蕴涵识别(Recognizing Textual Entailment，RTE)主要目标是对前提和假设进行判断，判断其是否具有蕴涵关系。文本蕴涵识别形式上是一个文本分类的问题，在上面这个例子中是一个三分类的问题，其label分别为entailment，contradiction，neutral。



### entailment的应用价值

#### 文本蕴涵是计算语言学领域最具挑战性的课题之一，也是众多自然语言处理应用的一个重要部分。文本蕴涵作为一种文本推理的通用框架，为整合各种文本的语义表示方法、知识获取方法和推理方法提供了平台，具有广泛的应用前景。 



## 方法

### RNN

#### 循环神经网络(Recurrent Neural Network, RNN)是一类以序列(sequence)数据为输入，在序列的演进方向进行递归(recursion)且所有节点(循环单元)按链式连接的递归神经网络(recursive neural network)。循环神经网络具有记忆性、参数共享并且图灵完备(Turing completeness)，因此在对序列的非线性特征进行学习时具有一定优势。循环神经网络在自然语言处理(Natural Language Processing, NLP)，例如语音识别、语言建模、机器翻译等领域有应用，也被用于各类时间序列预报。引入了卷积神经网络(Convoutional Neural Network，CNN)构筑的循环神经网络可以处理包含序列输入的计算机视觉问题。

#### RNN背后的思想是利用顺序信息。在传统的神经网络中，我们假设所有的输入(包括输出)之间是相互独立的。对于很多任务来说，这是一个非常糟糕的假设。如果你想预测一个序列中的下一个词，你最好能知道哪些词在它前面。RNN之所以循环的，是因为它针对系列中的每一个元素都执行相同的操作，每一个操作都依赖于之前的计算结果。换一种方式思考，可以认为RNN记忆了到当前为止已经计算过的信息。理论上，RNN可以利用任意长的序列信息，但实际中只能回顾之前的几步。下面是RNN的一个典型结构图：

![png2](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/2.png)


### LSTM

#### 长短期记忆网络(LSTM，Long Short-Term Memory)是一种时间循环神经网络，是为了解决一般的RNN(循环神经网络)存在的长期依赖问题而专门设计出来的，所有的RNN都具有一种重复神经网络模块的链式形式。在标准RNN中，这个重复的结构模块只有一个非常简单的结构，例如一个tanh层。

#### LSTM的内部结构看上去就是这样的一种效果，一个一个首尾相接，同一层的会把前面单元的输出作为后面单元的输入；前一层的输出会作为后一层的输入。

![png3](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/3.png)


#### LSTM 的关键就是细胞状态，水平线在图上方从左到右贯穿运行。细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互。左面的乘号是一个乘法操作，右面的加号就是普通的线性叠加、这个操作相当于左侧的Ct-1进入单元后，先被一个乘法器乘以一个系数后，再线性叠加一个数值然后从右侧输出去。

![png4](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/4.png)



### dropout

#### 随机失活(dropout)是对具有深度结构的人工神经网络进行优化的方法，在学习过程中通过将隐含层的部分权重或输出随机归零，降低节点间的相互依赖性(co-dependence )从而实现神经网络的正则化(regularization)，降低其结构风险(structural risk)。

#### 随机失活是为解决深度神经网络的过拟合(overfitting)和梯度消失(gradient vanishing)问题而被提出的优化方法，其一般设想是在神经网络的学习过程中，随机将部分隐含层节点的权重归零，由于每次迭代受归零影响的节点不同，因此各节点的“重要性”会被平衡。引入随机失活后，神经网络的每个节点都会贡献内容，不会出现少数高权重节点完全控制输出结果的情况，因此降低了网络的结构风险 。

#### 按神经网络自身的不同结构，随机失活的实现方法有差异。其中，对于本项目使用的循环神经网络(Recurrent Neural Network, RNN)，随机失活按网络的拓扑结构可以作用于每个时间步的输入和状态矩阵 。

#### 总之，具有大量参数的深度神经网络是非常强大的机器学习系统。然而，过拟合是这种网络中的严重问题。大型网络使用起来也很慢，因此在测试时通过组合许多不同的大型神经网络的预测很难处理过拟合。随机失活是一种解决此问题的技术。关键思想是在训练期间从神经网络中随机丢弃单位(及其连接)。这可以防止单位共适应太多。在训练期间，从指数数量的不同“稀疏”网络中抽取样本。在测试时，通过简单地使用具有较小权重的单个未加网络的网络，很容易近似平均所有这些稀疏网络的预测的效果。这显著减少了过拟合，并且比其他正则化方法有了重大改进。研究表明，随机失活提高了神经网络在视觉，语音识别，文档分类和计算生物学中监督学习任务的性能，在许多基准数据集上获得了最先进的结果。



#### 



## 测试

#### 1.测试语句1："Maurita and Jade both were at the scene of the car crash."

#### 测试语句2："Multiple people saw the accident."

#### 预期结果为正相关，测试结果也为正相关

![png5](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/5.png)

#### 2.测试语句1："There are two dogs playing in the park."

#### 测试语句2："A child saw three dogs in the park."

#### 预期结果为负相关，测试结果为负相关

![png6](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/6.png)


#### 3.测试语句1："My plane at 7 o'clock this morning."
#### 测试语句2："He came by train yesterday."

#### 预期结果为中立，测试结果为中立

![png7](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/7.png)


###

## 改进方案

#### 本项目采用了基于LSTM网络的模型来实现文本蕴含，使用的是斯坦福的SNLI数据集，并用GloVe模型来对数据集进行向量化表示。本项目虽然在英文文本蕴含方面有着不错的表现，但是却并不适用于中文文本蕴含。

#### 目前，中文文本蕴含大多采用机器学习的方法，通过人工提取大量特征构造分类器进行识别，这些方法需要依赖于特征工程以及大量的自然语言处理工具。

#### 对于中文文本蕴含识别任务，目前已经有许多种方法，包括基于规则的方法、基于相似度的方法、基于对齐的方法、基于机器学习的方法、基于深度神经网络的方法等。

#### 基于规则的方法需要由人工编写若干中文文本蕴含关系的规则,当满足某一规则时,给出是否蕴含的结论。基于规则的方法的优点是直观、识别准确、易于理解;缺点是规则的编写需要花费大量的人力与时间,由于中文表述的多样性以及背景知识的缺乏,规则并不能涵盖全部的语言现象。

#### 基于相似度的方法认为“相似即蕴含”,文本对之间的相似度越高，它们之间存在蕴含关系的可能性越大。在实验中会根据训练数据设定一个阈值，测试时，如果文本对的相似度高于阈值则判定为“蕴含”,否则认为“不蕴含”。基于相似度的方法的优点,是实现相对简单，可以判断在词汇层面是否具有蕴含关系;缺点是强行假设“相似即蕴含”，导致大量相似但并不蕴含的文本对被错误识别,也不能深人理解句法、语义关系。

#### 基于对齐的方法是在基于相似度的方法，上演化出来的,找出文本对之间的相似部分并通过对齐技术进行对齐,然后根据对齐的程度识别是否蕴含。基于对齐的方法的优点是直观;缺点是不够灵活,对具有复杂对齐方式的文本蕴含关系识别效果不佳。

#### 基于机器学习的方法通过人工已标注好的数据提取大量的词汇特征、句法特征、语义特征等，然后构造分类器(如SVM,LR等)进行分类。基于机器学习的方法的优点是适用于样本数据量小的情况，减少了规则的使用;缺点是需要人工提取大量特征，不仅耗时耗力,而且分类效果严重依赖提取的特征，并且在提取特征的时候需要使用大量自然语言处理工具,也会引入新的错误。

#### 随着深度神经网络技术在图像、语音等领域的成功应用,基于深度神经网络的方法在文本蕴含识别中的应用研究也逐渐增多。深度神经网络方法和传统方法相比,有如下几个特点。

##### (1)减少甚至避免人工参与。传统方法需要大量的人工抽取特征,深度神经网络可以避免传统机:器学习方法中的人工抽取特征工作。

##### (2)减少错误累计。传统方法需要词性标注、命名实体识别等NLP工具,而使用多种NLP工具时容易导致错误累计问题,深度神经网络的方法可以在一-定程度上减少错误累计。

##### (3)模型调整。方便传统方法的可塑性较深度神经网络方法低,如果用传统方法解决问题,改进成本巨大,调整模型时可能需要对代码进行大量改动。而深度神经网络的方法只需要调整参数,就可以调整模型,具有很强的灵活性和成长性。

##### (4)训练成本稍高。虽然深度神经网络方法较传统方法的训练成本高,但是当前高速发展的硬件性能可以支撑深度神经网络的训练。

### 在参考了网上的论文后，我们学习了一种新的中文文本蕴含识别方法——将CNN与双向LSTM相结合。首先对文本进行预处理，之后将句子映射到向量表示，再使用CNN和双向LSTM分别对文本进行编码，提取相关特征，避免人工筛选大量特征以及NLP工具造成的错误累计问题，然后使用全连接层进行分类得到初步的识别结果，最后使用语义规则对网络识别结果进行修正，得到最终的蕴含识别结果。

![png8](https://github.com/scusec/Data-Mining-for-Cybersecurity/blob/master/Homework/2019/Task1/7/img/8.png)
