{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from urllib.parse import unquote,quote\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train = []\n",
    "sql_train = []\n",
    "for x in open(\"./dataset/normal_train.csv\",'rb').readlines():\n",
    "    normal_train.append(unquote(x[:-1].decode()))\n",
    "for x in open(\"./dataset/sql_train.csv\",'rb').readlines():\n",
    "    sql_train.append(unquote(x[:-1].decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理特征(9个)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ent(domain):  # 计算信息熵\n",
    "    dataset = []\n",
    "    for each in domain:\n",
    "        dataset.append(each)\n",
    "    data1 = np.array(dataset)\n",
    "    x_value_list = set([data1[i] for i in range(data1.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(data1[data1 == x_value].shape[0]) / data1.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_feature(obj):\n",
    "    result = []\n",
    "    for domain in obj:\n",
    "        feature = []\n",
    "        key_num = 0\n",
    "        special_proportion = 0\n",
    "        prefix_proportion = 0\n",
    "        length = len(domain)  # 长度\n",
    "        if length == 0:\n",
    "            continue\n",
    "        num_proportion = len(re.compile(r'\\d').findall(domain))/length   # 数字比例\n",
    "        cap_proportion = len(re.compile(r'[A-Z]').findall(domain))/length  # 大写字母比例\n",
    "        space_proportion = domain.count(\" \")/length  # 空格比例\n",
    "        domain=domain.lower()\n",
    "        keyword = ['and','or','xor','sysobjects','version','substr','len','substring','exists','mid','asc','inner join','xp_cmdshell','exec'\\\n",
    "                  ,'having','union','order','information schema','load_file','load data infile','into outfile','into dumpfile','select']\n",
    "        special_char = ['{','}','(',')','NULL','=','?','[',']']\n",
    "        prefix = ['\\\\x','&','\\\\u','%']\n",
    "        for i in keyword:\n",
    "            key_num += domain.count(i)  # 关键词数量\n",
    "        for i in special_char:\n",
    "            special_proportion += domain.count(i)/length  # 特殊符号比例\n",
    "        for i in prefix:\n",
    "            prefix_proportion += domain.count(i)/length  # 前缀比例\n",
    "        close = (1 if (domain.count(\"'\")%2 == 0) & (domain.count(\"\\\"\")%2 == 0) else 0)  # 引号是否封闭\n",
    "        info = calc_ent(domain)  # 信息熵\n",
    "        feature = [length,num_proportion,cap_proportion,space_proportion,key_num,special_proportion,prefix_proportion,close,info]\n",
    "        result.append(feature)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正常域名： 5000\n",
      "SQL域名： 4974\n"
     ]
    }
   ],
   "source": [
    "print(\"正常域名：\",len(make_feature(normal_train)))\n",
    "print(\"SQL域名：\",len(make_feature(sql_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for feature in make_feature(sql_train):\n",
    "    features.append(feature)\n",
    "    labels.append(1)\n",
    "for feature in make_feature(normal_train):\n",
    "    features.append(feature)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K阶近邻及其K折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9488807216839291\n",
      "[0.83967936 0.9008016  0.94589178 0.9739479  0.97993982 0.95887663\n",
      " 0.95486459 0.94383149 0.98094283 0.89167503]\n",
      "0.9370451032456085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import cross_val_score \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=4)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_test, y_test))\n",
    "scores = cross_val_score(knn, features, labels, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.9993297587131368\n",
      "recall_score: 0.9933377748167888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\", lineno=245)\n",
    "train_X, test_X, train_y, test_y = train_test_split(features, labels, test_size=0.3, random_state=5)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_X, train_y)\n",
    "pred_y = clf.predict(test_X)\n",
    "print(\"precision_score:\", precision_score(y_true=test_y, y_pred=pred_y))\n",
    "print(\"recall_score:\", recall_score(y_true=test_y, y_pred=pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.9993297587131368\n",
      "recall_score: 0.9933377748167888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "decision_tree.fit(train_X, train_y)\n",
    "Y_pred = decision_tree.predict(test_X)\n",
    "print(\"precision_score:\", precision_score(y_true=test_y, y_pred=pred_y))\n",
    "print(\"recall_score:\", recall_score(y_true=test_y, y_pred=pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.9993297587131368\n",
      "recall_score: 0.9933377748167888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "log_reg = LogisticRegression(solver='liblinear', max_iter = 10000)\n",
    "log_reg.fit(train_X, train_y)\n",
    "Y_pred = log_reg.predict(test_X)\n",
    "print(\"precision_score:\", precision_score(y_true=test_y, y_pred=pred_y))\n",
    "print(\"recall_score:\", recall_score(y_true=test_y, y_pred=pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用KNN进行检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN测试：baidu\n",
      "Normal\n",
      "KNN测试：123\n",
      "Normal\n",
      "KNN测试：123abc\n",
      "Normal\n",
      "KNN测试：'and 1=1\n",
      "SQL\n",
      "KNN测试：?id=243234\n",
      "Normal\n",
      "KNN测试：?name=2eoyh\n",
      "Normal\n"
     ]
    }
   ],
   "source": [
    "def test_domain_KNN(domain):\n",
    "    test_feature = []\n",
    "    test_feature.append(domain)\n",
    "    sample = make_feature(test_feature)\n",
    "    pre = knn.predict(sample)\n",
    "    print(\"SQL\") if pre == 1 else print(\"Normal\")\n",
    "\n",
    "while True:\n",
    "    test_domain_KNN(input(\"KNN测试：\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
